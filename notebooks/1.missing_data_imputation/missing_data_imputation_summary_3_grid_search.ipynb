{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding The Best Feature Engineering Strategy Using sklearn GridSearchCV\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align:center\">\n",
    "<img src=\"images/search.jpg\" alt=\"search\" width=\"600\"/>\n",
    "<figcaption>Photo by <a href=\"https://unsplash.com/@laughayette?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Marten Newhall</a> on <a href=\"https://unsplash.com/s/photos/search?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></figcaption>\n",
    "</div>\n",
    "<br>  \n",
    "   \n",
    "We previously reviewed a few missing data imputation strategies using sklearn in [this post](), but which one should we use? How do we know which one works best for our data? Should we manually write a script to fit a model for each strategy and track the model performance? We could, but it would be a headache to track many different models, especially if we use cross validation to get more reliable experiment results. \n",
    "    \n",
    "Fortunately, sklearn offers great tools to streamline and optimize the process, which are `GridSearchCV` and `Pipeline`! You might be already familiar with using `GridSearchCV` for finding optimal hyperparameters of a model, but you might not be familiar with using it for finding optimal feature engineering strategies. \n",
    "    \n",
    "In this post, I would like to walk you through how `GridSearchCV` and `Pipeline` can be used to find the best feature engineering strategies for the given data. We will focus on missing data imputation strategies here but it can be used for any other feature engineering steps or combinations. \n",
    "\n",
    "# Table of Conents\n",
    "1. [Prepare Data](#prep)\n",
    "2. [Setup a Base Pipeline](#base)  \n",
    "3. [Finding The Best Imputation Technique Using GridSearchCV](#find_best)\n",
    "4. [References](#ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prep'></a>\n",
    "# 1. Prepare Data\n",
    "\n",
    "First, import necessary libraries and prepare data. We will use the [house price data from Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) in this post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# preparing data \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# feature scaling, encoding\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# putting together in pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# model selection\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import house price data \n",
    "df = pd.read_csv('../data/house_price/train.csv', index_col='Id')\n",
    "\n",
    "# find numerical columns vs. categorical columns, except for the target ('SalePrice')\n",
    "num_cols = df.drop('SalePrice', axis=1).select_dtypes('number').columns\n",
    "cat_cols = df.drop('SalePrice', axis=1).select_dtypes('object').columns\n",
    "\n",
    "# define X and y for GridSearchCV\n",
    "X = df.drop('SalePrice', axis=1)\n",
    "y = df['SalePrice']\n",
    "\n",
    "# split train and test dataset \n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('SalePrice', axis=1), \n",
    "                                                    df['SalePrice'], \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='base'></a>\n",
    "# 2. Setup a Base Pipeline\n",
    "\n",
    "<a id='pipeline'></a>\n",
    "## 2.1. Define Pipelines\n",
    "\n",
    "\n",
    "The next step is defining a base `Pipeline` for our model as below. \n",
    "\n",
    "1. Define two feature preprocessing pipelines; one for numerical variables (`num_pipe`) and the other for categorical variables (`cat_pipe`). `num_pipe` has `SimpleImputer` for missing data imputation and `StandardScaler` for scaling data. `cat_pipe` has `SimpleImputer` for missing data imputation and `OneHotEncoder` for encoding categorical data as numerical data. \n",
    "\n",
    "2. Combine those two pipelines together using `ColumnTransformer` to apply them to a different set of columns. \n",
    "\n",
    "3. Define the final pipeline called `pipe` by putting the `preprocess` pipeline together with an estimator, which is `Lasso` regression in this example. \n",
    "\n",
    "\n",
    "For details of this pipeline, please check out the previous post [Combining Feature Engineering and Model Fitting (Pipeline vs. ColumnTransformer)](). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering pipeline for numerical variables \n",
    "num_pipe= Pipeline([('imputer', SimpleImputer(strategy='mean', add_indicator=False)),\n",
    "                    ('scaler', StandardScaler())])\n",
    "\n",
    "# feature engineering pipeline for categorical variables \n",
    "# Note: fill_value='Missing' is not used for strategy='most_frequent' but defined here for later use\n",
    "cat_pipe = Pipeline([('imputer', SimpleImputer(strategy='most_frequent', fill_value='Missing')),\n",
    "                     ('encoder', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# put numerical and categorical feature engineering pipelines together\n",
    "preprocess = ColumnTransformer([(\"num_pipe\", num_pipe, num_cols),\n",
    "                                (\"cat_pipe\", cat_pipe, cat_cols)])\n",
    "\n",
    "\n",
    "# put transformers and an estimator together\n",
    "pipe = Pipeline([('preprocess', preprocess),\n",
    "                 ('lasso', Lasso(max_iter=10000))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"fit\"></a>\n",
    "## 2.2. Fit Pipeline\n",
    "\n",
    "Okay, so let's fit the model with our train data and test with the test data. Here, we get 0.63 for the score, which is $R^2$ of the prediction in this case ([sklearn Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso.score))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6308258188969262"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model \n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also cross validate the model using `cross_val_score`. It splits the whole data into 5 sets and calculates the score 5 times by fitting and testing with different sets each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85570392, 0.8228412 , 0.80381056, 0.88846653, 0.63236809])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross validate\n",
    "cross_val_score(pipe, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=params></a>\n",
    "## 2.3. Diffferent Parameters To Test\n",
    "\n",
    "Let's say we want to try different combinations of missing data imputation strategies for `SimpleImputer`, such as both `'median'` and `'mean'` for `strategy` and both `True` and `False` for `add_indicator`. To compare all of the cases, we need to test 4 different models with the following numerical variable imputation methods: \n",
    "\n",
    "    SimpleImputer(strategy='mean', add_indicator=False)\n",
    "    SimpleImputer(strategy='median', add_indicator=False)\n",
    "    SimpleImputer(strategy='mean', add_indicator=True)\n",
    "    SimpleImputer(strategy='median', add_indicator=True)\n",
    "\n",
    "We could copy and paste the script we wrote above, replace the corresponding step, and compare the performance of each case. It would not be too bad for the 4 combinations. But what if we want to test more combinations such as `strategy='constant'` and `strategy='most_frequent'` for categorical variables? Now it becomes 8 combinations ($2 \\times 2 \\times 2 = 8$). \n",
    "\n",
    "The more parameters we add, the more cases we have to test ad track (exponentially growing cases!). But don't worry! We have `GridSearchCV`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='find_best'></a>\n",
    "# 3. Finding The Best Imputation Technique Using GridSearchCV\n",
    "\n",
    "\n",
    "## 3.1. What Is GridSearchCV? \n",
    "`GridSearchCV` is a sklearn class that is used to find parameters with the best cross validation given the search space (parameter combinations). This can be used not only for hyperparameter tuning for estimators (e.g. `alpha` for Lasso), but also for parameters in any preprocessing step. We just need to define parameters that we want to optimize and pass them to `GridSearchCV()` as a dictionary. \n",
    "\n",
    "The rule for defining the grid search parameter key-value pair is the following:\n",
    "1. Key: a string that combines the name of the step with the name of the parameter with two understcores  \n",
    "2. Value: a list of parameter values to test \n",
    "\n",
    "In short, it's `{'step_name__parameter_name': a list of values}`. For example, if the step name is `lasso` and the parameter name is `alpha`, your grid search param becomes: \n",
    "`{'lasso__alph': [1, 5, 10]}` \n",
    "\n",
    "\n",
    "## 3.2. Defining Nested Parameters\n",
    "What about nested parameters that we have in our case? For example, our missing data imputation strategy for numerical variables is a few steps away from the final pipeline such as `preprocess` --> `num_pipe` --> `imputer`. \n",
    "\n",
    "Even for those cases, we can simply expand the key by keeping combining them with two unerstcore:\n",
    "\n",
    "`{'preprocess__num_pipe__imputer__strategy': ['mean', 'median', 'most_frequent']}`\n",
    "\n",
    "## 3.3. Defining and Fitting GridSearchCV\n",
    "\n",
    "With the basics of `GridSearchCV`, let's define `GridSearchCV` and its parameters for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the GridSearchCV parameters \n",
    "param = dict(preprocess__num_pipe__imputer__strategy=['mean', 'median', 'most_frequent'],\n",
    "             preprocess__num_pipe__imputer__add_indicator=[True, False],\n",
    "             preprocess__cat_pipe__imputer__strategy=['most_frequent', 'constant']) \n",
    "\n",
    "# define GridSearchCV \n",
    "grid_search = GridSearchCV(pipe, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to find the best parameters by simply running `fit`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('preprocess',\n",
       "                                        ColumnTransformer(transformers=[('num_pipe',\n",
       "                                                                         Pipeline(steps=[('imputer',\n",
       "                                                                                          SimpleImputer()),\n",
       "                                                                                         ('scaler',\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         Index(['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n",
       "       'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n",
       "       'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2nd...\n",
       "       'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature',\n",
       "       'SaleType', 'SaleCondition'],\n",
       "      dtype='object'))])),\n",
       "                                       ('lasso', Lasso(max_iter=10000))]),\n",
       "             param_grid={'preprocess__cat_pipe__imputer__strategy': ['most_frequent',\n",
       "                                                                     'constant'],\n",
       "                         'preprocess__num_pipe__imputer__add_indicator': [True,\n",
       "                                                                          False],\n",
       "                         'preprocess__num_pipe__imputer__strategy': ['mean',\n",
       "                                                                     'median',\n",
       "                                                                     'most_frequent']})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search the best parameters by fitting the GridSearchCV \n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Checking the results\n",
    "\n",
    "To check the combinations of parameters we tested and their performances in each cross validation set in terms of score and time, we can use the attribute `.cv_results`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([2.18816075, 2.09236193, 2.13147969, 2.50800061, 2.32593288,\n",
       "        2.27255459, 6.30061116, 6.08292165, 7.72316875, 6.17959538,\n",
       "        5.88832502, 5.95221062]),\n",
       " 'std_fit_time': array([2.33014303, 2.18218962, 2.21403329, 2.07129679, 2.15609185,\n",
       "        2.07633598, 2.80065069, 2.67514097, 4.33544705, 2.74682899,\n",
       "        2.60949829, 2.6094918 ]),\n",
       " 'mean_score_time': array([0.01650333, 0.01588135, 0.01679888, 0.01649656, 0.01653342,\n",
       "        0.015307  , 0.01581092, 0.0162437 , 0.02716937, 0.01591206,\n",
       "        0.01764565, 0.01588569]),\n",
       " 'std_score_time': array([0.00090241, 0.00043411, 0.00097888, 0.0017922 , 0.00083208,\n",
       "        0.00085008, 0.00040076, 0.00078959, 0.0101243 , 0.00106524,\n",
       "        0.00434013, 0.00186513]),\n",
       " 'param_preprocess__cat_pipe__imputer__strategy': masked_array(data=['most_frequent', 'most_frequent', 'most_frequent',\n",
       "                    'most_frequent', 'most_frequent', 'most_frequent',\n",
       "                    'constant', 'constant', 'constant', 'constant',\n",
       "                    'constant', 'constant'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_preprocess__num_pipe__imputer__add_indicator': masked_array(data=[True, True, True, False, False, False, True, True,\n",
       "                    True, False, False, False],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_preprocess__num_pipe__imputer__strategy': masked_array(data=['mean', 'median', 'most_frequent', 'mean', 'median',\n",
       "                    'most_frequent', 'mean', 'median', 'most_frequent',\n",
       "                    'mean', 'median', 'most_frequent'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'preprocess__cat_pipe__imputer__strategy': 'most_frequent',\n",
       "   'preprocess__num_pipe__imputer__add_indicator': True,\n",
       "   'preprocess__num_pipe__imputer__strategy': 'mean'},\n",
       "  {'preprocess__cat_pipe__imputer__strategy': 'most_frequent',\n",
       "   'preprocess__num_pipe__imputer__add_indicator': True,\n",
       "   'preprocess__num_pipe__imputer__strategy': 'median'},\n",
       "  {'preprocess__cat_pipe__imputer__strategy': 'most_frequent',\n",
       "   'preprocess__num_pipe__imputer__add_indicator': True,\n",
       "   'preprocess__num_pipe__imputer__strategy': 'most_frequent'},\n",
       "  {'preprocess__cat_pipe__imputer__strategy': 'most_frequent',\n",
       "   'preprocess__num_pipe__imputer__add_indicator': False,\n",
       "   'preprocess__num_pipe__imputer__strategy': 'mean'},\n",
       "  {'preprocess__cat_pipe__imputer__strategy': 'most_frequent',\n",
       "   'preprocess__num_pipe__imputer__add_indicator': False,\n",
       "   'preprocess__num_pipe__imputer__strategy': 'median'},\n",
       "  {'preprocess__cat_pipe__imputer__strategy': 'most_frequent',\n",
       "   'preprocess__num_pipe__imputer__add_indicator': False,\n",
       "   'preprocess__num_pipe__imputer__strategy': 'most_frequent'},\n",
       "  {'preprocess__cat_pipe__imputer__strategy': 'constant',\n",
       "   'preprocess__num_pipe__imputer__add_indicator': True,\n",
       "   'preprocess__num_pipe__imputer__strategy': 'mean'},\n",
       "  {'preprocess__cat_pipe__imputer__strategy': 'constant',\n",
       "   'preprocess__num_pipe__imputer__add_indicator': True,\n",
       "   'preprocess__num_pipe__imputer__strategy': 'median'},\n",
       "  {'preprocess__cat_pipe__imputer__strategy': 'constant',\n",
       "   'preprocess__num_pipe__imputer__add_indicator': True,\n",
       "   'preprocess__num_pipe__imputer__strategy': 'most_frequent'},\n",
       "  {'preprocess__cat_pipe__imputer__strategy': 'constant',\n",
       "   'preprocess__num_pipe__imputer__add_indicator': False,\n",
       "   'preprocess__num_pipe__imputer__strategy': 'mean'},\n",
       "  {'preprocess__cat_pipe__imputer__strategy': 'constant',\n",
       "   'preprocess__num_pipe__imputer__add_indicator': False,\n",
       "   'preprocess__num_pipe__imputer__strategy': 'median'},\n",
       "  {'preprocess__cat_pipe__imputer__strategy': 'constant',\n",
       "   'preprocess__num_pipe__imputer__add_indicator': False,\n",
       "   'preprocess__num_pipe__imputer__strategy': 'most_frequent'}],\n",
       " 'split0_test_score': array([0.8559258 , 0.85592588, 0.85592648, 0.85570392, 0.85551299,\n",
       "        0.85582335, 0.85879298, 0.85879279, 0.85878919, 0.85896052,\n",
       "        0.85895459, 0.85899454]),\n",
       " 'split1_test_score': array([0.82451561, 0.82451814, 0.82451547, 0.8228412 , 0.82308374,\n",
       "        0.82319623, 0.81847577, 0.81847707, 0.81847454, 0.81881294,\n",
       "        0.8188156 , 0.81871836]),\n",
       " 'split2_test_score': array([0.80452104, 0.80452115, 0.80452131, 0.80381056, 0.80386734,\n",
       "        0.80418338, 0.81387985, 0.81387993, 0.81387987, 0.8138799 ,\n",
       "        0.81388745, 0.81405991]),\n",
       " 'split3_test_score': array([0.88754205, 0.8875441 , 0.88754468, 0.88846653, 0.88861528,\n",
       "        0.8888325 , 0.88839025, 0.88839151, 0.88839181, 0.88872494,\n",
       "        0.8887452 , 0.88869994]),\n",
       " 'split4_test_score': array([0.63476757, 0.63476756, 0.63477279, 0.63236809, 0.63240312,\n",
       "        0.63500004, 0.64735875, 0.64735863, 0.64736569, 0.64747875,\n",
       "        0.64747512, 0.64859702]),\n",
       " 'mean_test_score': array([0.80145441, 0.80145537, 0.80145615, 0.80063806, 0.80069649,\n",
       "        0.8014071 , 0.80537952, 0.80537999, 0.80538022, 0.80557141,\n",
       "        0.80557559, 0.80581395]),\n",
       " 'std_test_score': array([0.08798175, 0.0879823 , 0.08798036, 0.08895383, 0.08895894,\n",
       "        0.08806601, 0.08361592, 0.08361623, 0.08361308, 0.08366907,\n",
       "        0.08367395, 0.08324655]),\n",
       " 'rank_test_score': array([ 9,  8,  7, 12, 11, 10,  6,  5,  4,  3,  2,  1], dtype=int32)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out the results\n",
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, which model did the `GridSearchCV` find to be most effective and what's its score? Let's check out `.bast_params_` and `.best_score_` attributes for that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preprocess__cat_pipe__imputer__strategy': 'constant',\n",
       " 'preprocess__num_pipe__imputer__add_indicator': False,\n",
       " 'preprocess__num_pipe__imputer__strategy': 'most_frequent'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out the best parameter combination found\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8058139542143075"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score \n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! It seems like using `constant` value for categorical variables and `most_frequent` values for numerical variables without missing indicator was found to be most effective in this case. Again, the best missing data imputation strategy depends on the data and the model. Try out with your data and see what works best for yours! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ref'></a>\n",
    "# References\n",
    "- [sklearn GridSearchCV]('https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html')\n",
    "- [sklearn Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "- [sklearn ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)\n",
    "- [Feature Engineering for Machine Learning](https://www.udemy.com/course/feature-engineering-for-machine-learning/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-feature-engineering",
   "language": "python",
   "name": "venv-feature-engineering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
